# Efficient Estimation of Word Representations in Vector Space(word2vec) - 论文阅读

Efficient Estimation of Word Representations in Vector Space - 基于向量空间中词表示的有效估计

本篇论文是 2013 年发表在 ICLR 上的论文。在NLP领域拥有里程碑式的意义，以至于后期的ELMo、Bert、GPT都是受词向量的影响而诞生的。同时本篇论文也是受到神经语言模型NNLM的启发。

- 出处：https://arxiv.org/abs/1301.3781

- 作者：Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean

- 单位：Google

- 发表年份：2013年

------

## 摘要

我们提出了两种创新的模型结构，用于生成来自庞大数据集的单词连续向量表示。这些表示的质量通过**单词相似性任务**进行评估，并与以往基于不同类型神经网络的最佳性能技术进行了比较。我们观察到，在计算成本显著降低的情况下，准确性显著提升，即便是从包含 16 亿个单词的数据集中学得高质量的单词向量也只需不到一天的时间。此外，我们证明这些向量在我们的测试集上展现出了领先水平，尤其在测量句法和语义单词相似性方面。

- 提出了两种新颖的模型结构用来计算词向量
- 采用一种词相似度的任务来评估对比词向量质量
- 大量降低模型计算量可以提升词向量的质量
- 在语意和句法任务重，本文提出的词向量是当前最好的效果



## 1 引言 

当前许多 NLP 系统和技术将单词视为原子单元，即单词之间没有相似性的概念，因为它们在词汇表中表示为索引。这种方法有几个优点——简单性、鲁棒性 并且 观察到在大量数据上训练的简单模型优于在较少数据上训练的复杂系统。例如用于统计语言建模的流行 N-gram 模型 ，如今，可以在几乎所有可用数据（数万亿个单词）上训练 N-gram。



然而，这种简单的技术在许多任务中都存在局限性。例如，自动语音识别的相关域内数据量是有限的—性能通常由高质量转录语音数据的大小（通常只有数百万个单词）决定。在机器翻译中，许多语言的现有语料库仅包含数十亿个单词或更少。因此，在某些情况下，简单地扩展基本技术不会带来任何重大进展，我们必须关注更先进的技术。



近年来，随着机器学习技术的进步，在更大的数据集上训练更复杂的模型已经成为可能，并且它们通常优于简单模型。最成功的概念可能是使用单词的分布式表示。例如，**基于神经网络的语言模型**显着优于 N-gram 模型。

### 1.1 论文目标

本文的主要目标是介绍用于从 **包含数十亿单词的庞大数据集 **中学习高质量 **词向量** 的技术。据我们所知，之前提出的架构都没有成功地训练过超过**几亿个单词**同时**单词向量的维数在 50 - 100 之间**的情况。



我们使用最近提出的技术来测量结果向量表示的质量，期望相似的单词可以彼此接近，同时单词可以具有多种相似度。早期在屈折语言的上下文中已经观察到了这一点 - 例如，名词可以有多个词尾，如果我们在原始向量空间的子空间中搜索相似的单词，就有可能找到具有相似词尾的单词。



有些惊讶的是，我们发现单词表示的相似性超出了简单的句法规则。使用 **词偏移技术（word offset technique）**，对词向量执行简单的代数运算，例如，向量（“King”）-向量（“Man”）+向量（“Woman”）会产生最接近的向量单词 Queen 的向量表示。

> 这里的 词偏移技术（word offset technique）是指 词向量之间的代数运算



在本文中，我们尝试通过开发新的模型架构来最大限度地提高这些向量运算的准确性，以保留单词之间的线性规律。我们设计了一个**新的综合测试集**来测量句法和语义规律，并表明许多这样的规律可以高精度地学习。此外，我们还讨论了训练时间和准确性 和 词向量的维度以及训练数据量的关系。



### 1.2 先前的研究成果

将单词表示为连续向量有着悠久的历史。

在文献 [1] 中提出了一个非常流行的模型架构，用于估计神经网络语言模型（NNLM）。

这个模型采用了**前馈神经网络结构**，包括一个线性投影层和一个非线性隐藏层，用于共同学习单词向量表示和统计语言模型。



在文献 [13, 14] 中提出了神经网络语言模型（NNLM）的另一种有趣的架构。

在这个架构中，

- 首先使用具有**单个隐藏层的神经网络**学习单词向量。
- 然后再使用词向量来训练 NNLM。

**因此，即使不构建完整的 NNLM，也可以学习词向量。**

在本研究中，我们直接扩展了这个架构，并**专注于第一步**，即**使用简单的模型学习单词向量**。



后来的研究表明，单词向量可以被用来显著改进和简化许多自然语言处理（NLP）应用 [4, 5, 29]。

对单词向量的估计是通过使用不同的模型架构，并在不同的语料库上进行训练来完成的，由此产生的一些单词向量已经可以用于未来的研究和比较。

然而，据我们所知，这些架构在训练过程中的计算成本明显较大，与 [13] 中提出的架构相比，除了使用对角权重矩阵的某些版本的对数双线性模型 [23]。







## 2  一些模型架构（前人的模型）

- LDA
- LSA
- 前馈神经网络
- 循环神经网络
- 并行网络计算

人们提出了许多不同类型的模型来估计单词的连续表示，包括众所周知的潜在语义分析（LSA）和潜在狄利克雷分配（LDA）。

在本文中，我们重点关注**由神经网络学习的分布式单词表示**，因为之前的研究表明，它们在保留单词之间的线性规律方面明显优于 LSA [20, 31]；此外，对于大型数据集，LDA的计算成本也变得非常昂贵。



与文献 [18] 类似，为了比较不同的模型架构，我们首先将模型的**计算复杂性**定义为**需要访问的参数数量**，以完全训练模型。

接下来，我们将尝试在**最大化准确性的同时最小化计算复杂性**。



对于以下所有模型，训练复杂度与以下公式成正比


$$
O=E\times T\times Q
$$
其中，E 是训练周期（epochs）的数量，T是训练集中的单词数量，而Q则根据每个模型架构进一步定义。

一般的选择是将训练周期E设置在3到50之间，而训练集中的单词数量T可以达到十亿。

所有模型都使用**随机梯度下降（stochastic gradient descent）**和**反向传播（backpropagation）**进行训练，这是一种常见的深度学习模型训练方法。[26]。



### 2.1 前馈神经网络语言模型（FFNNLM）

前馈神经网络语言模型包括输入层（input layer）、投影层（projection layer）、隐藏层（hidden layer）和输出层（output layer）。

- 输入层：在输入层，前 N 个单词使用 1-of-V 编码进行编码，其中 V 是词汇表的大小。
- 投影层：输入层的信息被投影到一个投影层 P，该层的维度是 N × D，其中 N 是前 N 个单词，D 是投影维度。这个投影操作使用一个共享的投影矩阵进行。由于每次只有 N 个输入是活跃的，所以对投影层的组合是一个相对便宜的操作。
- 隐藏层：投影层的输出进一步传递到隐藏层，该层使用激活函数（如ReLU或sigmoid）引入非线性。隐藏层的大小通常是一个可调参数，决定了模型能够捕捉的语言结构的复杂性。隐藏层的大小记为 H
- 输出层：隐藏层的输出被用来计算对整个词汇表中每个单词的条件概率分布。通常，输出层采用 softmax 函数，将隐藏层的输出转换为概率分布。输出层的维度记为 V。



由于投影层中的值很密集，因此 NNLM 架构对于投影和隐藏层之间的计算会非常复杂。对于 $N = 10$ 的常见选择，投影层 P 的大小可能为 500 到 2000，而隐藏层 H 的大小通常为 500 到 1000 个单位。此外，隐藏层用于计算词汇表中所有单词的概率分布，从而产生维度为 V 的输出层。因此，每个训练示例的计算复杂度可以表示为：
$$
Q = N × D + N × D × H + H × V
$$

其中主导项是 $H × V$ 。然而，为了避免这种情况，提出了几种实用的解决方案；

- 要么使用softmax的分层版本[25,23,18]，

- 要么通过使用训练期间未标准化的模型来完全避免标准化模型[4,9]。

- 使用词汇表的二叉树表示，需要评估的输出单元的数量可以减少到 $log_2(V)$ 左右。因此，大部分复杂性是由 $N × D × H$ 项引起的。



在我们的模型中，我们使用**分层 softmax**，其中词汇表表示为**Huffman 二叉树**。

这是根据之前的观察得出的，即单词的频率对于在神经网络语言模型中获取类别非常有效[16]。

**Huffman 树的优势：** Huffman 树的构建使得频繁单词得到了较短的二进制编码，进一步减少了需要评估的输出单元的数量。与平衡的二叉树相比，Huffman 树的输出单元数量的评估约为 $log_2(Unigram Perplexity(V))$，其中 V 是词汇表的大小。

例如，当词汇表的大小为一百万个单词时，这将导致在评估中加速大约两倍。尽管对于神经网络语言模型来说，这并非至关重要的加速，因为计算的瓶颈在于 $N × D × H$ 这一项，但我们将在后续提出的架构中，这些架构不包含隐藏层，因此在很大程度上依赖于 softmax 归一化的效率。



### 2.2 循环神经网络语言模型（RNNLM）

基于**循环神经网络**的语言模型已经被提出来克服**前馈神经网络** NNLM 的某些局限性，例如需要**指定上下文长度**（模型的阶数 N ），并且因为理论上 RNN 可以比浅层神经网络有效地表示更复杂的模式网络 [15, 2]。 **RNN模型没有投影层；只有输入层、隐藏层和输出层**。此类模型的特殊之处在于使用延时连接将隐藏层与其自身连接的循环矩阵。这允许循环模型形成某种短期记忆，因为过去的信息可以由隐藏层状态表示，该状态根据当前输入和前一个时间步骤中隐藏层的状态进行更新。

RNN 模型每个训练示例的复杂度为：
$$
Q=H\times H+H\times V
$$
其中单词表示 $D$ 与隐藏层 $H$  具有相同的维度。同样，通过使用分层 softmax，项 $H × V$ 可以有效地简化为 $H × log_2(V )$。大部分复杂性来自 $H × H$。



### 2.3 神经网络的并行训练

为了在**巨大的数据集**上训练模型，我们在名为 DistBelief [6] 的大规模分布式框架之上实现了多个模型，包括**前馈 NNLM** 和 **本文提出的新模型**。该框架允许我们**并行运行同一模型的多个副本**，并且每个副本通过保留所有参数的集中式服务器同步其梯度更新。对于这种并行训练，我们使用**小批量异步梯度下降和称为 Adagrad 的自适应学习率**过程 [7]。在此框架下，通常使用一百个或更多模型副本，每个副本在数据中心的不同机器上使用许多 CPU 核心。



## 3 新的对数线性模型（本文的模型）

> New Log-linear Models

在本节中，我们提出了两种新的模型架构，用于学习单词的分布式表示，试图最大限度地减少计算复杂性。上一节的主要观察结果是，大部分复杂性是由模型中的**非线性隐藏层**引起的。虽然这就是神经网络如此有吸引力的原因，但我们决定探索更简单的模型，**这些模型可能无法像神经网络那样精确地表示数据，但可能可以有效地对更多数据进行训练**。



新的架构直接遵循我们早期工作[13, 14]中提出的架构，其中发现神经网络语言模型可以通过**两个步骤**成功训练：首先，使用**简单模型学习连续词向量**，然后**使用 N- gram NNLM 在这些单词的分布式表示之上进行训练**。虽然后来有大量工作集中在学习词向量上，但我们认为[13]中提出的方法是最简单的一种。请注意，相关模型也很早就被提出了[26, 8]。



### 3.1 CBOW 连续词袋模型

> CBOW 连续词袋模型

第一个提出的架构**类似于前馈 NNLM**，其中**非线性隐藏层被删除**，投影层为**所有单词（而不仅仅是投影矩阵）共享**；因此，所有单词都被投影到相同的位置（它们的向量被平均）。我们将这种架构称为**词袋模型**，因为历史中的单词顺序不会影响投影。此外，我们还使用未来的词语；通过构建一个输入有四个未来词和四个历史词的对数线性分类器，我们在下一节介绍的任务中获得了最佳性能，其中训练标准是正确分类当前（中间）词。训练复杂度为:
$$
Q = N × D + D × log_2(V )
$$
各个字母代表以下含义：

- \(Q\)：训练复杂度（Training Complexity）。
- \(N\)：训练样本的数量。
- \(D\)：词向量的维度。
- \(V\)：词汇表的大小（即不同单词的数量）。



我们将该模型进一步表示为 CBOW，因为与标准词袋模型不同，它使用上下文的连续分布式表示。模型架构如**图 1** 所示。请注意，输入层和投影层之间的权重矩阵以**与 NNLM 中相同的方式**为所有单词位置共享。



### 3.2 连续Skip-gram模型

> 连续Skip-gram模型

第二种架构与 CBOW 类似，但它不是根据上下文来预测当前单词，而是尝试根据同一句子中的另一个单词来最大化单词的分类。更准确地说，我们使用每个**当前单词**（中心词） 作为具有连续投影层的对数线性分类器的输入，并预测当前单词之前和之后一定范围内的单词（上下文）。我们发现增加范围可以提高生成的词向量的质量，但也增加了计算复杂性。由于距离较远的单词通常与当前单词的相关性低于那些接近的单词，因此我们通过在训练示例中从这些单词中采样较少的样本来减少对距离较远的单词的权重。



该架构的训练复杂度为：
$$
Q = C × (D + D × log_2(V ))
$$
各个字母代表以下含义：

- \(Q\)：训练复杂度（Training Complexity）。
- \(C\)：上下文窗口的大小，表示在当前词周围的单词范围内进行预测的窗口大小。
- \(D\)：词向量的维度。
- \(V\)：词汇表的大小（即不同单词的数量）。

因此，如果我们选择 $C = 5$ ，对于每个训练单词，我们将在 $< 1 , C>$ 范围内随机选择一个数字 $R$，然后使用当前单词的之前的 $R$ 个单词和 之后的 $R$ 个单词作为 正确的标签 。这将要求我们进行 $R × 2$  单词分类，以当前单词作为输入，将每个 $R + R$ 单词作为输出。在下面的实验中，我们使用 $C=10$。



> 图 1：新模型架构。 CBOW 架构根据上下文预测当前单词，而 Skip-gram 则在给定当前单词的情况下预测上下文。

![图 1：新模型架构。 CBOW 架构根据上下文预测当前单词，而 Skip-gram 则在给定当前单词的情况下预测周围的单词。](./assets/word2vec 论文精读.assets/image-20231203101300445.png)

## 4 实验结果

> 结果

为了比较不同版本词向量的质量，以前的论文通常使用一个表格来显示示例词及其最相似的词，并直观地理解它们。尽管很容易证明 "法国"一词与"意大利" 以及其他一些国家相似，但当将这些向量置于更复杂的相似性任务中时，就更具挑战性，如下所示。

我们根据之前的观察发现，单词之间可能存在许多不同类型的相似性，例如，单词“big”与“bigger”相似，就像“small”与“smaller”相似一样。另一种关系类型的示例可以是单词对“big - biggest”和“small - smallest”[20]。我们进一步将两对具有相同关系的单词表示为一个问题，就像我们可以问的那样：“What is the word that is similar to small in the same sense as biggest is similar to big?” - 当bigger 和 big 相似的情况下，什么单词和 small 相似。



有点令人惊讶的是，这些问题可以通过使用单词的向量表示执行简单的代数运算来回答。要找到一个与“small”相似的单词，就像“最大”与“大”相似一样，我们可以简单地计算向量 : 

$X = vector(biggest) − vector(big) + vector(small)$。

然后，我们在向量空间中搜索**通过余弦距离测量最接近 X 的单词**，并将其用作问题的答案（在搜索过程中我们丢弃输入的问题词）。当词向量经过良好训练后，可以使用此方法找到正确答案 - "smallest"。



最后，我们发现，当我们在大量数据上训练高维词向量时，得到的向量可以用来回答单词之间非常微妙的语义关系，例如城市和它所属的国家，例如法国之于巴黎，德国之于柏林。具有这种语义关系的词向量可用于改进许多现有的 NLP 应用程序，例如机器翻译、信息检索和问答系统，并且可能使其他尚未发明的未来应用程序成为可能。

![image-20231203102629464](./assets/word2vec 论文精读.assets/image-20231203102629464.png)

> 表 1：语义句法词关系测试集中五种语义问题和九种句法问题的示例。



### 4.1 任务描述

> 任务说明

为了衡量词向量的质量，我们定义了一个综合测试集，其中包含五种语义问题和九种语法问题。表 1 显示了每个类别的两个示例。总体而言，共有 8869 个语义问题和 10675 个句法问题。每个类别中的问题都是通过两个步骤创建的：首先，手动创建相似单词对的列表。然后，通过连接两个单词对形成一个大问题列表。例如，我们列出了 68 个美国大城市及其所属州，并通过随机选择两个单词对形成了大约 2500 个问题。我们在测试集中仅包含单个标记词，因此不存在多词实体（例如New York）。



我们评估所有问题类型的整体准确性，并分别评估每种问题类型（语义、句法）。仅当使用上述方法计算出的向量最接近的单词与问题中的正确单词**完全相同**时，才认为问题得到了正确回答；因此，同义词被视为错误。这也意味着**达到 100% 的准确率可能是不可能的**，因为当前的模型没有任何关于词形态的输入信息。然而，我们认为词向量对于某些应用程序的有用性应该与该准确性指标正相关。通过合并有关单词结构的信息，尤其是句法问题，可以取得进一步的进展。



### 4.2 最大化准确率

> 最大化准确性



我们使用**谷歌新闻语料库**来训练词向量。该语料库包含大约 6B 个标记（tokens）。我们将词汇量限制为 100 万个最常见的单词。显然，我们面临着时间受限的优化问题，因为可以预期使用更多数据和更高维度的词向量都会提高准确性。为了估计模型架构的最佳选择以快速获得尽可能好的结果，我们首先评估了在训练数据子集上训练的模型，词汇量限制为最常见的 30k 个单词。使用 CBOW 架构、选择不同的词向量维度和增加训练数据量的结果如 **表 2** 所示。

![image-20231203103917969](./assets/word2vec 论文精读.assets/image-20231203103917969.png)

>表 2：使用 CBOW 架构中词汇量有限的词向量，语义-句法词关系测试集子集的准确性。仅使用包含最常见的 30k 单词中的单词的问题。



可以看出，在某个时间点之后，添加更多维度或添加更多训练数据所带来的改进是递减的。因此，我们必须同时增加向量维度和训练数据量。虽然这一观察似乎微不足道，但必须指出的是，目前流行的是在相对大量的数据上训练词向量，但数据量不够（例如 50 - 100）。根据公式 4，训练数据量增加两倍会导致计算复杂度的增加与向量大小增加两倍大致相同。



### 4.3 模型架构比较

> 模型架构比较

首先，我们比较使用相同训练数据和 640 个词向量的相同维度导出词向量的不同模型架构。在进一步的实验中，我们使用新的语义句法词关系测试集中的全套问题，即不限制于 30k 词汇量。我们还包括[20]中引入的测试集的结果，该测试集重点关注单词之间的句法相似性。



训练数据由多个 LDC 语料库组成，在[18]中详细描述（320M 单词，82K 词汇量）。我们使用这些数据与之前训练的循环神经网络语言模型进行比较，该模型在单个 CPU 上训练大约需要 8 周。我们使用 DistBelief 并行训练 [6] 训练具有相同数量 640 个隐藏单元的前馈 NNLM，使用 8 个先前单词的历史记录（因此，NNLM 比 RNNLM 具有更多参数，因为投影层的大小为 640 × 8 ）。



在**表 3** 中可以看出，来自 RNN（如[20]中使用的）的词向量主要在句法问题上表现良好。 NNLM 向量的性能明显优于 RNN - 这并不奇怪，因为 RNNLM 中的词向量直接连接到非线性隐藏层。 CBOW 架构在句法任务上比 NNLM 表现更好，在语义任务上也差不多。最后，Skip-gram 架构在句法任务上的表现比 CBOW 模型稍差（但仍然比 NNLM 更好），而在测试的语义部分则比所有其他模型好得多。

![image-20231203104810226](./assets/word2vec 论文精读.assets/image-20231203104810226.png)

>表 3：使用在相同数据上训练的模型（具有 640 维词向量）的架构比较。准确性在我们的语义句法单词关系测试集和 [20] 的句法关系测试集上报告





接下来，我们评估了仅使用一个 CPU 训练的模型，并将结果与公开可用的词向量进行了比较。比较结果如 **表 4** 所示。CBOW 模型在 Google 新闻数据子集上的训练时间约为一天，而 Skip-gram 模型的训练时间约为三天。对于进一步报告的实验，我们仅使用一个训练周期（同样，我们线性降低学习率，使其在训练结束时接近零）。使用一个 epoch 使用两倍的数据训练模型所得到的结果与迭代三个 epoch 的相同数据相当或更好，如 **表 5** 所示，并提供了额外的小幅加速。

![image-20231203104925029](./assets/word2vec 论文精读.assets/image-20231203104925029-1571766.png)

> 表 4：语义句法词关系测试集上公开可用的词向量与我们模型中的词向量的比较。使用完整的词汇表。



![image-20231203104950478](./assets/word2vec 论文精读.assets/image-20231203104950478.png)

> 表 5：在相同数据上训练三个时期的模型与训练一个时期的模型的比较。准确性是在完整的语义-句法数据集上报告的。



### 4.4 模型的大规模并行训练

> 模型的大规模并行训练



如前所述，我们在名为 **DistBelief 的分布式框架** 中实现了各种模型。下面我们报告了在 Google News 6B 数据集上训练的几个模型的结果，这些模型采**用小批量异步梯度下降**和称为 **Adagrad 的自适应学习率过程** [7]。我们在训练期间使用了 50 到 100 个模型副本。 CPU 核心的数量是估计值，因为数据中心机器与其他生产任务共享，并且使用情况可能会波动很大。请注意，由于分布式框架的开销，CBOW 模型和 Skip-gram 模型的 CPU 使用率比它们的单机实现更加接近。结果报告于**表6**中。

![image-20231203105936296](./assets/word2vec 论文精读.assets/image-20231203105936296.png)

> 表 6：使用 DistBelief 分布式框架训练的模型的比较。请注意，使用 1000 维向量训练 NNLM 需要很长时间才能完成。





### 4.5 Microsoft研究院语句完成挑战

> Microsoft研究语句完成挑战

最近推出了 Microsoft Sentence Completion Challenge，作为一项推进语言建模和其他 NLP 技术的任务 [32]。该任务由 1040 个句子组成，每个句子中缺少一个单词，目标是在给出五个合理选择的列表的情况下选择与句子其余部分最一致的单词。已经在该集合上报告了几种技术的性能，包括 N-gram 模型、基于 LSA 的模型 [32]、对数双线性模型 [24] 以及目前拥有最先进性能的循环神经网络的组合该基准测试的准确度为 55.4% [19]。



我们探索了 **Skip-gram 架构**在此任务上的性能。首先，我们在[32]中提供的 50M 个单词上训练 640 维模型。然后，我们使用输入中的未知单词计算测试集中每个句子的分数，并预测句子中所有周围的单词。最终的句子得分是这些单独预测的总和。使用句子分数，我们选择最有可能的句子。



**表 7** 中列出了一些先前结果以及新结果的简短摘要。虽然 Skip-gram 模型本身在此任务上的表现并不比 LSA 相似性更好，但该模型的分数与 RNNLM 获得的分数是互补的，并且加权组合带来了新的最先进结果 58.9% 的准确率（该集的开发部分为 59.2%，该集的测试部分为 58.7%）。

![image-20231203110223206](./assets/word2vec 论文精读.assets/image-20231203110223206.png)

> 表 7：Microsoft 句子完成挑战赛模型的比较和组合。



## 5 Learned Relationships 的例子

>学习到词与词之间的关系



**表 8** 显示了遵循各种关系的单词。我们遵循上述方法：通过减去两个词向量来定义关系，并将结果添加到另一个词。例如，巴黎 - 法国 + 意大利 = 罗马。可以看出，准确度相当不错，尽管显然还有很大的进一步改进空间（请注意，使用我们假设完全匹配的准确度指标，表 8 中的结果得分仅为 60% 左右）。我们相信，在更大维度的数据集上训练的词向量将表现得更好，并将促进新的创新应用程序的开发。提高准确性的另一种方法是提供多个关系示例。通过使用十个例子而不是一个来形成关系向量（我们将各个向量平均在一起），我们观察到我们的最佳模型在语义句法测试中的准确度绝对提高了约 10%。

![image-20231203110638716](./assets/word2vec 论文精读.assets/image-20231203110638716.png)

> 表 8：单词对关系的示例，使用表 4 中的最佳单词向量（Skipgram 模型在 300 维的 783M 单词上进行训练）。



还可以应用向量运算来解决不同的任务。例如，通过计算单词列表的平均向量并找到最远的单词向量，我们观察到选择列表外单词的准确性很高。这是某些人类智力测试中常见的问题类型。显然，使用这些技术仍有很多发现有待实现。



## 6 结论

> 结论



在本文中，我们研究了在句法和语义语言任务集合上通过各种模型导出的单词向量表示的质量。我们观察到，与流行的神经网络模型（前馈和循环）相比，可以**使用非常简单的模型架构来训练高质量的词向量**。由于计算复杂度低得多，因此可以从更大的数据集计算非常准确的高维词向量。使用 **DistBelief 分布式框架**，即使在具有一万亿个单词的语料库上训练 CBOW 和 Skip-gram 模型也应该是可能的，词汇量基本上是无限的。这比之前发布的类似模型的最佳结果大几个数量级。

> DistBelief 是谷歌（Google）在2012年发布的一个分布式机器学习系统。它是由谷歌研究员使用以及推动深度学习研究和应用的一个工具。DistBelief早期用于实现谷歌的大规模深度学习模型，包括用于图像识别、语音识别等各种应用。



**SemEval-2012** 任务 2 [11] 是一项有趣的任务，其中词向量最近被证明显着优于先前的技术水平。公开可用的 RNN 向量与其他技术一起使用，使 Spearman 的排名相关性比之前的最佳结果提高了 50% 以上 [31]。基于神经网络的词向量之前已应用于许多其他 NLP 任务，例如情感分析 [12] 和释义检测 [28]。可以预期这些应用程序可以从本文描述的模型架构中受益。

>SemEval-2012（Semantic Evaluation）是一个国际性的语义评估任务。SemEval 是一个系列的评估比赛，旨在推动自然语言处理（NLP）和计算语言学（CL）领域的研究和发展。SemEval 的每一届都集中在不同的语义任务上，参与者需要使用机器学习和自然语言处理技术来解决这些任务。



我们正在进行的工作表明，词向量可以成功应用于知识库中事实的自动扩展，也可以用于验证现有事实的正确性。机器翻译实验的结果看起来也非常有希望。将来，将我们的技术与潜在关系分析 [30] 和其他技术进行比较也会很有趣。我们相信，我们的综合测试集将帮助研究界改进估计词向量的现有技术。我们还期望高质量的词向量将成为未来 NLP 应用的重要构建模块。





## 7 后续工作

> 后续工作

在撰写本文的初始版本后，我们发布了用于计算词向量的[单机多线程 C++ 代码](https://github.com/tmikolov/word2vec)，使用连续词袋和 Skip-Gram 架构4。训练速度明显高于本文前面报道的速度，即对于典型的超参数选择，训练速度约为每小时数十亿字。我们还发布了超过 140 万个代表命名实体的向量，并接受了超过 1000 亿个单词的训练。我们的一些后续工作将发表在即将发表的 NIPS 2013 论文中 [21]。
